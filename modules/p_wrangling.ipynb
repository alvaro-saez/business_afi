{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0741461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import bs4\n",
    "import re\n",
    "from datetime import datetime\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "#to send emails\n",
    "import email, smtplib, ssl\n",
    "from email import encoders\n",
    "from email.mime.base import MIMEBase\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "#to make the conection with spreadsheets\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "#for passwords\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c178582b",
   "metadata": {},
   "source": [
    "# B. p_wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ef17ba",
   "metadata": {},
   "source": [
    "## Create the DATAFRAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58731bf",
   "metadata": {},
   "source": [
    "### a) Full DataFrame with a daily injection of the new scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b5f9eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spreadsheet conection done\n"
     ]
    }
   ],
   "source": [
    "def conection_spreadsheet(scopes, credentials_location):\n",
    "    scopes = scopes\n",
    "    credentials = Credentials.from_service_account_file(\n",
    "        credentials_location,\n",
    "        scopes=scopes\n",
    "    )\n",
    "    gc = gspread.authorize(credentials)\n",
    "    #If we want to host our credentials in a server we have to do the next code:\n",
    "    # gc = gspread.service_account(filename='https://www.path/to/the/downloaded/file.json')\n",
    "    print(\"spreadsheet conection done\")\n",
    "    return gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8715a",
   "metadata": {},
   "source": [
    "#### Step 1: Create or define the dataFrame\n",
    "\n",
    " - From a CSV if it exists\n",
    " - If not, from spreadsheets\n",
    " - If not, from this notebook\n",
    " - If not, we have to create a empty dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32133a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_single created through csv\n"
     ]
    }
   ],
   "source": [
    "def main_df(location,gc):\n",
    "#1. From a CSV if it exists\n",
    "    try:\n",
    "        df_single = pd.read_csv(location)\n",
    "        print(\"df_single created through csv\")\n",
    "        return df_single\n",
    "    \n",
    "#2. If not, from spreadsheets    \n",
    "    except: #if the except is executed, the try has given an error, so it does not exist in csv (we use the spreadsheets backup)   \n",
    "        try:\n",
    "            sheet = gc.open(\"business_afi_scraping_df_single\").sheet1  #Abrir spreadhseet\n",
    "            data_from_spreadsheets = sheet.get_all_records()  #Obtener todos los registros\n",
    "            df_single = pd.DataFrame(data_from_spreadsheets)\n",
    "            print(\"df_single created through spreadsheet\")\n",
    "            return df_single\n",
    "        \n",
    "#3. If not, from this notebook\n",
    "        except: #if the except is executed, the try has given an error, so let's see if it was already created in the notebook       \n",
    "            try:\n",
    "                df_single.head()\n",
    "                print(\"df_single created through this notebook\")\n",
    "                return df_single\n",
    "            \n",
    "#4. If not, we have to create a empty dataFrame\n",
    "            except: #if it gives an error, it does not exist in the notebook and we will create it from scratch\n",
    "                columns = [\"date_hyphen\",\"date_slash\",\"date_number\",\"product_name\",\"product_id\",\"product_brand\",\"price\",\"status\",\"url\"]\n",
    "                df_single = pd.DataFrame(columns=columns)\n",
    "                print(\"df_single created through zero\")\n",
    "                return df_single\n",
    "            else: #if there is no error, it exists in the notebook and we will do nothing\n",
    "                pass\n",
    "            \n",
    "        else: #if it doesn't give an error, it exists in spreadsheets and we won't do anything\n",
    "            pass\n",
    "            \n",
    "    else: #f it doesn't give an error, it exists in csv and we won't do anything\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f784fb5b",
   "metadata": {},
   "source": [
    "#### Step 2: Append new records in the df_single dataFrame - pd.concat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31cb3825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_append_new_files(final_date_hyphen_products,final_date_slash_products,final_date_number_products,final_name_products,final_id_products,final_brand_products,final_price_products_list,final_price_products_status,urls_products_list):\n",
    "    df_append_new_files = pd.DataFrame({\n",
    "        \"date_hyphen\":final_date_hyphen_products,\n",
    "        \"date_slash\":final_date_slash_products,\n",
    "        \"date_number\":final_date_number_products,\n",
    "        \"product_name\":final_name_products,\n",
    "        \"product_id\":final_id_products,\n",
    "        \"product_brand\":final_brand_products,\n",
    "        \"price\":final_price_products_list,\n",
    "        \"status\":final_price_products_status,\n",
    "        \"url\": urls_products_list\n",
    "    })\n",
    "    df_append_new_files[\"price\"] = df_append_new_files[\"price\"].astype(\"int64\")\n",
    "    \n",
    "    return df_append_new_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94015d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_df(df_single,df_append_new_files):\n",
    "    if df_single.empty == True:\n",
    "        df_single = pd.concat([df_single, df_append_new_files], ignore_index=True)\n",
    "        return df_single\n",
    "    elif df_append_new_files[\"date_hyphen\"][0] == df_single[\"date_hyphen\"][len(df_single)-1]:\n",
    "        return df_single\n",
    "    elif df_append_new_files[\"date_hyphen\"][0] != df_single[\"date_hyphen\"][len(df_single)-1]:\n",
    "        df_single = pd.concat([df_single, df_append_new_files], ignore_index=True)\n",
    "        return df_single"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d5d73f",
   "metadata": {},
   "source": [
    "### b) Clean and Prepare the dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fda3b008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corregir_gam_name(url,product_name):\n",
    "    if url == \"https://sillasybienestar.com/gaming/sillas-gaming/review-individual/intimate-wm-heart-gaming/\" and product_name == \"none\":\n",
    "        return \"intimate wm heat  Racing Silla Gamer\"\n",
    "    else:\n",
    "        return product_name\n",
    "        \n",
    "def corregir_gam_id(url,product_id):\n",
    "    if url == \"https://sillasybienestar.com/gaming/sillas-gaming/review-individual/intimate-wm-heart-gaming/\" and product_id == \"none\":\n",
    "        return \"B075CK3GVJ\"\n",
    "    else:\n",
    "        return product_id\n",
    "        \n",
    "def corregir_gam_brand(url,product_brand):\n",
    "    if url == \"https://sillasybienestar.com/gaming/sillas-gaming/review-individual/intimate-wm-heart-gaming/\" and product_brand == \"none\":\n",
    "        return \"intimate wm heat\"\n",
    "    else:\n",
    "        return product_brand\n",
    "\n",
    "    \n",
    "def corregir_gam_name_hbada_rep(url,product_name):\n",
    "    if url == \"https://sillasybienestar.com/ergonomia/sillas-ergonomicas/review-individual/hbada-reposabrazos/\" and product_name == \"none\":\n",
    "        return \"Hbada B07V51M94R\"\n",
    "    else:\n",
    "        return product_name\n",
    "        \n",
    "def corregir_gam_id_hbada_rep(url,product_id):\n",
    "    if url == \"https://sillasybienestar.com/ergonomia/sillas-ergonomicas/review-individual/hbada-reposabrazos/\" and product_id == \"none\":\n",
    "        return \"B07V51M94R\"\n",
    "    else:\n",
    "        return product_id\n",
    "        \n",
    "def corregir_gam_brand_hbada_rep(url,product_brand):\n",
    "    if url == \"https://sillasybienestar.com/ergonomia/sillas-ergonomicas/review-individual/hbada-reposabrazos/\" and product_brand == \"none\":\n",
    "        return \"Hbada\"\n",
    "    else:\n",
    "        return product_brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f84afb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction_df_single(df_single):\n",
    "    #intimate wm heat  Racing Silla Gamer\n",
    "    df_single[\"product_name\"] = df_single.apply(lambda df_single: corregir_gam_name(df_single[\"url\"], df_single[\"product_name\"]), axis=1)\n",
    "    df_single[\"product_id\"] = df_single.apply(lambda df_single: corregir_gam_id(df_single[\"url\"], df_single[\"product_id\"]), axis=1)\n",
    "    df_single[\"product_brand\"] = df_single.apply(lambda df_single: corregir_gam_brand(df_single[\"url\"], df_single[\"product_brand\"]), axis=1)\n",
    "\n",
    "    #Hbada B07V51M94R\n",
    "    df_single[\"product_name\"] = df_single.apply(lambda df_single: corregir_gam_name_hbada_rep(df_single[\"url\"], df_single[\"product_name\"]), axis=1)\n",
    "    df_single[\"product_id\"] = df_single.apply(lambda df_single: corregir_gam_id_hbada_rep(df_single[\"url\"], df_single[\"product_id\"]), axis=1)\n",
    "    df_single[\"product_brand\"] = df_single.apply(lambda df_single: corregir_gam_brand_hbada_rep(df_single[\"url\"], df_single[\"product_brand\"]), axis=1)\n",
    "\n",
    "    return df_single"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649c8009",
   "metadata": {},
   "source": [
    "### c) Create the price main of each product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbc5acf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_mean_dic(df_single):\n",
    "    product_id_list_mean = df_single[\"product_id\"].unique().tolist()\n",
    "    mean_dic = {}\n",
    "    for i in product_id_list_mean:\n",
    "        mean_dic[i] = int(round(df_single[df_single[\"product_id\"] == i][\"price\"].mean(),0))\n",
    "    return mean_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a58ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_mean_dic_to_df(df_single,product_mean):\n",
    "    df_single[\"product_mean\"] = df_single[\"product_id\"].apply(lambda x: product_mean[x])\n",
    "    return df_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79ed8fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_mean_status_func_values(product_price,product_mean):\n",
    "    if product_price < product_mean and product_price > 0:\n",
    "        return \"precio por debajo de la media\"\n",
    "    elif product_price > product_mean and product_price > 0:\n",
    "        return \"precio por encima de la media\"\n",
    "    elif product_price == product_mean and product_price > 0:\n",
    "        return \"precio igual que la media\"\n",
    "    elif product_price == 0:\n",
    "        return \"producto descatalogado o sin stock\"\n",
    "    else:\n",
    "        return \"revisar, aviso\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "974a89ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_mean_status_func_apply(df_single):\n",
    "    df_single[\"product_mean_status\"] = df_single.apply(lambda x: product_mean_status_func_values(x[\"price\"], x[\"product_mean\"]), axis=1)\n",
    "    return df_single"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59523a4",
   "metadata": {},
   "source": [
    "### d) Create the category of each product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8a0a5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categories_url(product_url):\n",
    "    if \"/ergonomia/sillas-ergonomicas/review-individual/\" in product_url:\n",
    "        return \"ergonomia\"\n",
    "    elif \"/oficina-y-escritorio/sillas-de-oficina/review-individual/\" in product_url:\n",
    "        return \"oficina\"\n",
    "    elif \"/ergonomia/sillas-de-rodillas/review-individual/\" in product_url:\n",
    "        return \"rodilla\"\n",
    "    elif \"/gaming/sillas-gaming/review-individual/\" in product_url:\n",
    "        return \"gaming\"\n",
    "    else:\n",
    "        return \"alerta sin categoria\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9926b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_category(df_single):\n",
    "    df_single[\"product_category\"] = df_single[\"url\"].apply(categories_url)\n",
    "    return df_single"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55532e1e",
   "metadata": {},
   "source": [
    "## EXPORT the dataFrames to CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cd5532e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'files exported successfully'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def export_files(df_single, df_append_new_files):\n",
    "    df_single_to_csv = df_single.to_csv(\"../files/df_single.csv\", sep=\",\", index=False)\n",
    "    df_append_new_files_to_csv = df_append_new_files.to_csv(\"../files/df_append_new_files_last_day.csv\", sep=\",\", index=False)#+str(datetime.today().strftime('%Y-%m-%d'))+\".csv\", sep=\",\", index=False)\n",
    "    out_of_stock_df = df_append_new_files[df_append_new_files[\"status\"] != \"correcto\"]\n",
    "    out_of_stock_df_to_csv = out_of_stock_df.to_csv(\"../files/out_of_stock_last_day.csv\", sep=\",\", index=False)\n",
    "    \n",
    "    none_values = df_single[df_single[\"product_name\"]==\"none\"].any().unique().tolist()\n",
    "    if none_values == [True]:\n",
    "        none_values_df = df_single[df_single[\"product_name\"]==\"none\"]\n",
    "        none_values_df_to_csv = none_values_df.to_csv(\"../files/non_values_last_day.csv\", sep=\",\", index=False)\n",
    "    \n",
    "    return \"files exported successfully\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5226aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ironhack]",
   "language": "python",
   "name": "conda-env-ironhack-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
